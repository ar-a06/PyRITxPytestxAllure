{"uid":"6586db82122866ae","name":"test_racial_bias_with_predefined_prompts","fullName":"tests.test_bias_fairness#test_racial_bias_with_predefined_prompts","historyId":"3c2080574f1b1be6503df0b66930129d","time":{"start":1738336663908,"stop":1738337072340,"duration":408432},"status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-b3VPqI3avWEHvKJXXHqps9cf on tokens per min (TPM): Limit 100000, Used 92051, Requested 8172. Please try again in 1h36m20.16s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}","statusTrace":"@pytest.mark.asyncio\n    async def test_racial_bias_with_predefined_prompts():\n        racial_bias_tester = Racial_bias()\n>       memory, score_memory =await racial_bias_tester.test_racial_bias_with_predefined_prompts()\n\ntests\\test_bias_fairness.py:69: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nobject_model\\bias_fairness\\racial_bias.py:70: in test_racial_bias_with_predefined_prompts\n    await self.orchestrator.send_prompts_async(\n.venv\\Lib\\site-packages\\pyrit\\orchestrator\\prompt_sending_orchestrator.py:105: in send_prompts_async\n    return await self.send_normalizer_requests_async(\n.venv\\Lib\\site-packages\\pyrit\\orchestrator\\prompt_sending_orchestrator.py:129: in send_normalizer_requests_async\n    responses: list[PromptRequestResponse] = await self._prompt_normalizer.send_prompt_batch_to_target_async(\n.venv\\Lib\\site-packages\\pyrit\\prompt_normalizer\\prompt_normalizer.py:124: in send_prompt_batch_to_target_async\n    return await batch_task_async(\n.venv\\Lib\\site-packages\\pyrit\\common\\batch_helper.py:87: in batch_task_async\n    batch_results = await asyncio.gather(*tasks)\n.venv\\Lib\\site-packages\\pyrit\\prompt_normalizer\\prompt_normalizer.py:60: in send_prompt_async\n    response = await target.send_prompt_async(prompt_request=request)\n.venv\\Lib\\site-packages\\pyrit\\prompt_target\\common\\utils.py:26: in set_max_rpm\n    return await func(*args, **kwargs)\n.venv\\Lib\\site-packages\\pyrit\\prompt_target\\openai\\openai_chat_target.py:106: in send_prompt_async\n    resp_text = await self._complete_chat_async(messages=messages)\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:418: in exc_check\n    raise retry_exc.reraise()\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:185: in reraise\n    raise self.last_attempt.result()\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n.venv\\Lib\\site-packages\\pyrit\\prompt_target\\openai\\openai_chat_target.py:217: in _complete_chat_async\n    response: ChatCompletion = await self._async_client.chat.completions.create(\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1702: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1849: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1543: in request\n    return await self._request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1629: in _request\n    return await self._retry_request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1676: in _retry_request\n    return await self._request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1629: in _request\n    return await self._retry_request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1676: in _retry_request\n    return await self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019287C66AB0>, cast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeo...00, 'n': 1, 'presence_penalty': 0.0, 'seed': None, 'stream': False, 'temperature': 1.0, 'top_p': 1.0}, extra_json=None)\n\n    async def _request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool,\n        stream_cls: type[_AsyncStreamT] | None,\n        retries_taken: int,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = await self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        await self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        try:\n            response = await self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Request: %s %s \"%i %s\"', request.method, request.url, response.status_code, response.reason_phrase\n        )\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                await err.response.aclose()\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                await err.response.aread()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-b3VPqI3avWEHvKJXXHqps9cf on tokens per min (TPM): Limit 100000, Used 92051, Requested 8172. Please try again in 1h36m20.16s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1644: RateLimitError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1738335502203,"stop":1738335502203,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false},{"name":"event_loop","time":{"start":1738336663908,"stop":1738336663908,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false}],"testStage":{"status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-b3VPqI3avWEHvKJXXHqps9cf on tokens per min (TPM): Limit 100000, Used 92051, Requested 8172. Please try again in 1h36m20.16s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}","statusTrace":"@pytest.mark.asyncio\n    async def test_racial_bias_with_predefined_prompts():\n        racial_bias_tester = Racial_bias()\n>       memory, score_memory =await racial_bias_tester.test_racial_bias_with_predefined_prompts()\n\ntests\\test_bias_fairness.py:69: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nobject_model\\bias_fairness\\racial_bias.py:70: in test_racial_bias_with_predefined_prompts\n    await self.orchestrator.send_prompts_async(\n.venv\\Lib\\site-packages\\pyrit\\orchestrator\\prompt_sending_orchestrator.py:105: in send_prompts_async\n    return await self.send_normalizer_requests_async(\n.venv\\Lib\\site-packages\\pyrit\\orchestrator\\prompt_sending_orchestrator.py:129: in send_normalizer_requests_async\n    responses: list[PromptRequestResponse] = await self._prompt_normalizer.send_prompt_batch_to_target_async(\n.venv\\Lib\\site-packages\\pyrit\\prompt_normalizer\\prompt_normalizer.py:124: in send_prompt_batch_to_target_async\n    return await batch_task_async(\n.venv\\Lib\\site-packages\\pyrit\\common\\batch_helper.py:87: in batch_task_async\n    batch_results = await asyncio.gather(*tasks)\n.venv\\Lib\\site-packages\\pyrit\\prompt_normalizer\\prompt_normalizer.py:60: in send_prompt_async\n    response = await target.send_prompt_async(prompt_request=request)\n.venv\\Lib\\site-packages\\pyrit\\prompt_target\\common\\utils.py:26: in set_max_rpm\n    return await func(*args, **kwargs)\n.venv\\Lib\\site-packages\\pyrit\\prompt_target\\openai\\openai_chat_target.py:106: in send_prompt_async\n    resp_text = await self._complete_chat_async(messages=messages)\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:418: in exc_check\n    raise retry_exc.reraise()\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:185: in reraise\n    raise self.last_attempt.result()\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n.venv\\Lib\\site-packages\\pyrit\\prompt_target\\openai\\openai_chat_target.py:217: in _complete_chat_async\n    response: ChatCompletion = await self._async_client.chat.completions.create(\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1702: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1849: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1543: in request\n    return await self._request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1629: in _request\n    return await self._retry_request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1676: in _retry_request\n    return await self._request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1629: in _request\n    return await self._retry_request(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1676: in _retry_request\n    return await self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019287C66AB0>, cast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeo...00, 'n': 1, 'presence_penalty': 0.0, 'seed': None, 'stream': False, 'temperature': 1.0, 'top_p': 1.0}, extra_json=None)\n\n    async def _request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool,\n        stream_cls: type[_AsyncStreamT] | None,\n        retries_taken: int,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = await self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        await self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        try:\n            response = await self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Request: %s %s \"%i %s\"', request.method, request.url, response.status_code, response.reason_phrase\n        )\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                await err.response.aclose()\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                await err.response.aread()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-b3VPqI3avWEHvKJXXHqps9cf on tokens per min (TPM): Limit 100000, Used 92051, Requested 8172. Please try again in 1h36m20.16s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1644: RateLimitError","steps":[],"attachments":[{"uid":"4b725921725aaab","name":"log","source":"4b725921725aaab.txt","type":"text/plain","size":5489}],"parameters":[],"attachmentStep":false,"hasContent":true,"stepsCount":0,"attachmentsCount":1,"shouldDisplayMessage":true},"afterStages":[{"name":"event_loop::3","time":{"start":1738337072415,"stop":1738337072416,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false},{"name":"event_loop::_close_event_loop","time":{"start":1738337072416,"stop":1738337072416,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false},{"name":"event_loop::_restore_policy","time":{"start":1738337072416,"stop":1738337072416,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false},{"name":"event_loop::_provide_clean_event_loop","time":{"start":1738337072416,"stop":1738337072416,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false}],"labels":[{"name":"tag","value":"asyncio"},{"name":"parentSuite","value":"tests"},{"name":"suite","value":"test_bias_fairness"},{"name":"host","value":"AP00003340"},{"name":"thread","value":"15684-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"tests.test_bias_fairness"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":true,"retry":true,"extra":{"categories":[],"tags":["asyncio"]},"source":"6586db82122866ae.json","parameterValues":[]}